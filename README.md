## Technology Highlights
- Python 3
- Elasticsearch
- Kibana 5

## Objective
Data mining. Programmatically correlate massive data sets (millions) from several external API sources. Inherently decrease manual intervention and productivity up to 90%.

## About the Project
**Python**

Small modules have been developed to maintain integrity of this project. A sequence of modules, classes and definitions are executed to control the sequence in which data is processed. This entire project is written in Python 3 from scratch. Using a Dell PowerEdge

**Elasticsearch**

All data is written to Elastics distributed database used for offline processing. The use of Elastics indices, aliases, document types and mappings makes large data sets easily searchable.

**Kibana**

Used to visualize data and allow others to interact with the system.

## Overview
1. Data Collection 
Pull massive data sets from several API endpoints and write to an Elasticsearch cluster. The raw data is slightly modified as it flows through the system, making it easier to work with.

2. Data Reduction
Uniqueness is found between complex data sets in an effort to minimize resource consumption. This data can be referenced at a later time, without the need for reprocessing.

3. Data Correlation and Analysis
A set of rules are defined programmatically, outlining how each fragment of data should be found, translated, correlated, standardized then processed. This step is critical to maintain data integrity.
